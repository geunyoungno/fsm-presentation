/**
 * XStateë¡œ LLM í˜¸ì¶œí•˜ê¸°
 *
 * í•µì‹¬ í†µì°°: LLM í˜¸ì¶œë„ ì¼ë°˜ì ì¸ ë¹„ë™ê¸° ì‘ì—…ê³¼ ë™ì¼í•œ íŒ¨í„´!
 * fetch-example.tsì™€ ë¹„êµí•˜ë©´ì„œ í•™ìŠµí•˜ì„¸ìš”.
 */

import { createMachine, createActor, assign, fromPromise } from 'xstate';
import { loadEnv, isOpenAIAvailable } from './env.js';
import { callOpenAI } from './openai-client.js';

// í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
const env = loadEnv();

const LLM_CALL_DELAY_MS = 1000;
const MAX_LLM_RETRIES = 3;
const LLM_SUCCESS_RATE = 0.9;
const RETRY_DELAY_MS = 1500;

interface ChatContext {
  message: string;
  response: string | null;
  error: string | null;
  retryCount: number;
  conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }>;
}

type ChatEvent =
  | { type: 'SEND_MESSAGE'; message: string }
  | { type: 'RETRY' };

/**
 * Mock LLM ì‘ë‹µ ìƒì„±
 */
const callLLMMock = async (message: string): Promise<string> => {
  // API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜ (1ì´ˆ ì§€ì—°)
  await new Promise(resolve => setTimeout(resolve, LLM_CALL_DELAY_MS));

  // ì‹¤íŒ¨ í™•ë¥  ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ LLM API ì—ëŸ¬ ì‹œë®¬ë ˆì´ì…˜)
  if (Math.random() > LLM_SUCCESS_RATE) {
    throw new Error('LLM API í˜¸ì¶œ ì‹¤íŒ¨: íƒ€ì„ì•„ì›ƒ');
  }

  // ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±
  const responses: Record<string, string> = {
    'hello': 'ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?',
    'fsm': 'FSM(Finite State Machine)ì€ ìœ í•œí•œ ê°œìˆ˜ì˜ ìƒíƒœë¡œ ì‹œìŠ¤í…œì„ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.',
    'typescript': 'TypeScriptëŠ” JavaScriptì— íƒ€ì… ì‹œìŠ¤í…œì„ ì¶”ê°€í•œ ì–¸ì–´ì…ë‹ˆë‹¤.',
    'default': `"${message}"ì— ëŒ€í•œ ì‘ë‹µì…ë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.`
  };

  const lowerMessage = message.toLowerCase();
  for (const [key, value] of Object.entries(responses)) {
    if (lowerMessage.includes(key)) {
      return value;
    }
  }

  return responses['default'];
};

/**
 * LLM í˜¸ì¶œ (í™˜ê²½ ë³€ìˆ˜ì— ë”°ë¼ ì‹¤ì œ API ë˜ëŠ” Mock ì‚¬ìš©)
 *
 * OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ì‹¤ì œ OpenAI APIë¥¼ í˜¸ì¶œí•˜ê³ ,
 * ì—†ìœ¼ë©´ Mock ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
 */
const callLLM = async (
  conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }>
): Promise<string> => {
  if (isOpenAIAvailable()) {
    // ì‹¤ì œ OpenAI API í˜¸ì¶œ
    return await callOpenAI(conversationHistory, env.OPENAI_MODEL || 'gpt-4o-mini');
  } else {
    // Mock ì‘ë‹µ ë°˜í™˜
    const lastMessage = conversationHistory
      .filter(entry => entry.role === 'user')
      .pop()?.content || '';
    return await callLLMMock(lastMessage);
  }
};

const chatMachine = createMachine({
  id: 'chat',
  types: {} as {
    context: ChatContext;
    events: ChatEvent;
  },
  initial: 'idle',
  context: {
    message: '',
    response: null,
    error: null,
    retryCount: 0,
    conversationHistory: []
  },
  states: {
    idle: {
      entry: () => console.log('ğŸ’¬ [Idle] ë©”ì‹œì§€ ì…ë ¥ ëŒ€ê¸° ì¤‘...'),
      on: {
        SEND_MESSAGE: {
          target: 'calling_llm',
          actions: assign({
            message: ({ event }) => event.message,
            response: null,
            error: null,
            retryCount: 0,
            conversationHistory: ({ context, event }) => [
              ...context.conversationHistory,
              { role: 'user' as const, content: event.message }
            ]
          })
        }
      }
    },
    calling_llm: {
      entry: () => console.log('ğŸ¤– [Calling LLM] LLM í˜¸ì¶œ ì¤‘...'),
      /**
       * í•µì‹¬ íŒ¨í„´: invoke + fromPromise
       *
       * âœ¨ fetch-example.tsì™€ ë¹„êµ:
       *
       * fetch-example.ts:
       *   invoke: {
       *     src: fromPromise(async () => {
       *       return await fetch('/api/users/1');
       *     })
       *   }
       *
       * llm-chat.ts (í˜„ì¬ íŒŒì¼):
       *   invoke: {
       *     src: fromPromise(async () => {
       *       return await callLLMSimulated(message);
       *     })
       *   }
       *
       * â†’ ì™„ì „íˆ ë™ì¼í•œ íŒ¨í„´!
       * XStateëŠ” "ë¬´ì—‡ì„ í˜¸ì¶œí•˜ëŠ”ê°€"ì— ë¬´ê´€ì‹¬í•©ë‹ˆë‹¤.
       * REST APIë“  LLM APIë“  ëª¨ë‘ ë™ì¼í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
       */
      invoke: {
        src: fromPromise(async ({ input }: { input: ChatContext }) => {
          return await callLLM(input.conversationHistory);
        }),
        input: ({ context }) => context,
        onDone: {
          target: 'success',
          actions: assign({
            response: ({ event }) => event.output,
            conversationHistory: ({ context, event }) => [
              ...context.conversationHistory,
              { role: 'assistant' as const, content: event.output }
            ]
          })
        },
        onError: {
          target: 'error',
          actions: assign({
            error: ({ event }) => event.error instanceof Error ? event.error.message : String(event.error)
          })
        }
      }
    },
    success: {
      entry: ({ context }) => {
        console.log('âœ… [Success] LLM ì‘ë‹µ ìˆ˜ì‹ ');
        console.log(`\nì‚¬ìš©ì: ${context.message}`);
        console.log(`LLM: ${context.response}\n`);
      },
      on: {
        SEND_MESSAGE: {
          target: 'calling_llm',
          actions: assign({
            message: ({ event }) => event.message,
            response: null,
            error: null,
            conversationHistory: ({ context, event }) => [
              ...context.conversationHistory,
              { role: 'user' as const, content: event.message }
            ]
          })
        }
      }
    },
    error: {
      entry: ({ context }) => {
        console.log(`âŒ [Error] LLM í˜¸ì¶œ ì‹¤íŒ¨: ${context.error}`);
        console.log(`ì¬ì‹œë„ íšŸìˆ˜: ${context.retryCount}/${MAX_LLM_RETRIES}`);
      },
      /**
       * LLM ì¬ì‹œë„ ë¡œì§
       *
       * LLM API í˜¸ì¶œì€ ë‹¤ì–‘í•œ ì´ìœ ë¡œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
       * - ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ
       * - API ìš”ìœ¨ ì œí•œ (Rate Limit)
       * - ì„œë²„ ê³¼ë¶€í•˜
       *
       * XStateì˜ guardë¥¼ ì‚¬ìš©í•˜ì—¬ ì¬ì‹œë„ ì¡°ê±´ì„ ëª…í™•íˆ ì •ì˜í•©ë‹ˆë‹¤.
       */
      after: {
        [RETRY_DELAY_MS]: [
          {
            guard: ({ context }) => context.retryCount < MAX_LLM_RETRIES,
            target: 'calling_llm',
            actions: [
              assign({
                retryCount: ({ context }) => context.retryCount + 1,
                error: null
              }),
              () => console.log('ğŸ”„ ì¬ì‹œë„ ì¤‘...')
            ]
          },
          {
            target: 'failed'
          }
        ]
      },
      on: {
        RETRY: {
          target: 'calling_llm',
          actions: assign({
            retryCount: ({ context }) => context.retryCount + 1,
            error: null
          })
        }
      }
    },
    failed: {
      entry: () => console.log('ğŸš« [Failed] ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ - ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'),
      type: 'final'
    }
  }
});

// ì‹¤í–‰
console.log('=== XState LLM Chat Example ===\n');
console.log('ğŸ’¡ í•µì‹¬: LLM í˜¸ì¶œë„ ì¼ë°˜ ë¹„ë™ê¸° ì‘ì—…ê³¼ ë™ì¼í•œ íŒ¨í„´ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.\n');
console.log('ğŸ“ fetch-example.tsì™€ ë¹„êµí•´ë³´ì„¸ìš”!\n');

const chatActor = createActor(chatMachine);

chatActor.subscribe((state) => {
  console.log(`í˜„ì¬ ìƒíƒœ: ${state.value}`);
});

chatActor.start();

// ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜
setTimeout(() => {
  chatActor.send({
    type: 'SEND_MESSAGE',
    message: 'FSMì´ ë­ì•¼?'
  });
}, 1000);

// ì²« ë²ˆì§¸ ì‘ë‹µì„ ê¸°ë‹¤ë¦° í›„ ë‘ ë²ˆì§¸ ì§ˆë¬¸
setTimeout(() => {
  chatActor.send({
    type: 'SEND_MESSAGE',
    message: 'TypeScriptì— ëŒ€í•´ ì•Œë ¤ì¤˜'
  });
}, 10000); // 10ì´ˆë¡œ ì¦ê°€

// í”„ë¡œê·¸ë¨ ì¢…ë£Œ
setTimeout(() => {
  console.log('\nâœ¨ ëŒ€í™” ì™„ë£Œ\n');
  chatActor.stop();
  process.exit(0);
}, 20000); // 20ì´ˆë¡œ ì¦ê°€
