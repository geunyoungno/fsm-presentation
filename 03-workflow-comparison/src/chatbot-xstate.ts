/**
 * XStateë¥¼ ì‚¬ìš©í•œ LLM ì±—ë´‡ ì›Œí¬í”Œë¡œìš°
 * ìƒíƒœ ë¨¸ì‹ ì˜ ëª…í™•í•œ ìƒíƒœ ì „ì´ì™€ íƒ€ì… ì•ˆì „ì„± ê°•ì¡°
 */

import { createMachine, createActor, assign, fromPromise } from 'xstate';
import { loadEnv, isOpenAIAvailable, callOpenAI } from './env.js';

// í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
const env = loadEnv();

// ìƒìˆ˜ ì •ì˜
const LLM_CALL_DELAY_MS = 1000;
const MAX_LLM_RETRIES = 3;
const LLM_SUCCESS_RATE = 0.8; // 80% ì„±ê³µë¥  (Mock ì „ìš©)
const RETRY_DELAY_MS = 1500;

interface ChatContext {
  userMessage: string;
  conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }>;
  currentResponse: string | null;
  error: string | null;
  retryCount: number;
}

type ChatEvent =
  | { type: 'SEND_MESSAGE'; message: string }
  | { type: 'RETRY' }
  | { type: 'RESET' };

/**
 * Mock LLM ì‘ë‹µ ìƒì„±
 */
const callLLMMock = async (
  conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }>
): Promise<string> => {
  console.log('ğŸ¤– [Calling LLM Mock] Mock API í˜¸ì¶œ ì¤‘...');
  await new Promise(resolve => setTimeout(resolve, LLM_CALL_DELAY_MS));

  // ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜
  if (Math.random() > LLM_SUCCESS_RATE) {
    throw new Error('LLM API í˜¸ì¶œ ì‹¤íŒ¨: Rate limit exceeded');
  }

  // ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
  const lastUserMessage = conversationHistory
    .filter(msg => msg.role === 'user')
    .pop()?.content || '';

  const responses: Record<string, string> = {
    'hello': 'ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?',
    'fsm': 'FSM(Finite State Machine)ì€ ì‹œìŠ¤í…œì„ ëª…í™•í•œ ìƒíƒœë“¤ê³¼ ê·¸ë“¤ ê°„ì˜ ì „ì´ë¡œ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê³  í…ŒìŠ¤íŠ¸í•˜ê¸° ì‰¬ìš´ ì½”ë“œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
    'xstate': 'XStateëŠ” JavaScript/TypeScriptë¥¼ ìœ„í•œ ê°•ë ¥í•œ ìƒíƒœ ë¨¸ì‹  ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. íƒ€ì… ì•ˆì „ì„±ê³¼ ì‹œê°í™” ë„êµ¬ë¥¼ ì œê³µí•˜ë©°, ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ëª…í™•í•˜ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
    'workflow': 'ì›Œí¬í”Œë¡œìš°ëŠ” ì‘ì—…ì˜ ìˆœì„œì™€ ì¡°ê±´ì„ ì •ì˜í•œ ê²ƒì…ë‹ˆë‹¤. XState, Mastra, LangGraph ê°™ì€ ë„êµ¬ë“¤ì´ ê°ê° ë‹¤ë¥¸ ì² í•™ìœ¼ë¡œ ì›Œí¬í”Œë¡œìš°ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.',
    'default': `"${lastUserMessage}"ì— ëŒ€í•œ ì§ˆë¬¸ì´êµ°ìš”. êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë¶€ë¶„ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?`
  };

  const lowerMessage = lastUserMessage.toLowerCase();
  for (const [key, value] of Object.entries(responses)) {
    if (lowerMessage.includes(key)) {
      return value;
    }
  }

  return responses['default'];
};

/**
 * LLM í˜¸ì¶œ (í™˜ê²½ ë³€ìˆ˜ì— ë”°ë¼ ì‹¤ì œ API ë˜ëŠ” Mock ì‚¬ìš©)
 */
const callLLM = async (
  conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }>
): Promise<string> => {
  if (isOpenAIAvailable()) {
    // ì‹¤ì œ OpenAI API í˜¸ì¶œ
    return await callOpenAI(conversationHistory, env.OPENAI_MODEL || 'gpt-4o-mini');
  } else {
    // Mock ì‘ë‹µ ë°˜í™˜
    return await callLLMMock(conversationHistory);
  }
};

/**
 * XState ì±—ë´‡ ë¨¸ì‹ 
 *
 * ìƒíƒœ íë¦„:
 * waiting_input â†’ validating â†’ calling_llm â†’ response_ready
 *                                    â†“
 *                                  error â†’ (ì¬ì‹œë„ or ì‹¤íŒ¨)
 */
const chatbotMachine = createMachine({
  id: 'chatbot',
  types: {} as {
    context: ChatContext;
    events: ChatEvent;
  },
  initial: 'waiting_input',
  context: {
    userMessage: '',
    conversationHistory: [],
    currentResponse: null,
    error: null,
    retryCount: 0
  },
  states: {
    waiting_input: {
      entry: () => console.log('ğŸ’¬ [Waiting Input] ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸° ì¤‘...'),
      on: {
        SEND_MESSAGE: {
          target: 'validating',
          actions: assign({
            userMessage: ({ event }) => event.message,
            error: null,
            retryCount: 0
          })
        }
      }
    },
    validating: {
      entry: ({ context }) => console.log(`ğŸ” [Validating] ì…ë ¥ ê²€ì¦ ì¤‘: "${context.userMessage}"`),
      /**
       * XStateì˜ ê°•ì : always ì „ì´ë¡œ ë™ê¸° ê²€ì¦ ë¡œì§ì„ ëª…í™•í•˜ê²Œ í‘œí˜„
       *
       * - ì…ë ¥ì´ ë¹„ì–´ìˆìœ¼ë©´ ì¦‰ì‹œ waiting_inputìœ¼ë¡œ ë³µê·€
       * - ìœ íš¨í•˜ë©´ calling_llmìœ¼ë¡œ ì§„í–‰
       */
      always: [
        {
          guard: ({ context }) => context.userMessage.trim().length === 0,
          target: 'waiting_input',
          actions: () => console.log('âš ï¸ [Validation Failed] ë¹ˆ ë©”ì‹œì§€ëŠ” ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤')
        },
        {
          target: 'calling_llm'
        }
      ]
    },
    calling_llm: {
      entry: () => console.log('ğŸš€ [Calling LLM] LLM í˜¸ì¶œ ì‹œì‘'),
      /**
       * XState íŒ¨í„´: invoke + fromPromise
       *
       * í•µì‹¬ í†µì°°:
       * - 02-xstate-examples/llm-chat.tsì™€ ë™ì¼í•œ íŒ¨í„´
       * - ë¹„ë™ê¸° ì‘ì—…(LLM API)ì„ ìƒíƒœ ë¨¸ì‹ ì—ì„œ ê´€ë¦¬
       * - onDone: ì„±ê³µ ì‹œ ë‹¤ìŒ ìƒíƒœë¡œ ì „ì´
       * - onError: ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ì²˜ë¦¬ ìƒíƒœë¡œ ì „ì´
       */
      invoke: {
        src: fromPromise(async ({ input }: { input: ChatContext }) => {
          // ëŒ€í™” íˆìŠ¤í† ë¦¬ì— í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
          const updatedHistory = [
            ...input.conversationHistory,
            { role: 'user' as const, content: input.userMessage }
          ];
          const response = await callLLM(updatedHistory);
          return { response, updatedHistory };
        }),
        input: ({ context }) => context,
        onDone: {
          target: 'response_ready',
          actions: assign({
            currentResponse: ({ event }) => event.output.response,
            conversationHistory: ({ event }) => [
              ...event.output.updatedHistory,
              { role: 'assistant' as const, content: event.output.response }
            ]
          })
        },
        onError: {
          target: 'error',
          actions: assign({
            error: ({ event }) =>
              event.error instanceof Error ? event.error.message : String(event.error)
          })
        }
      }
    },
    response_ready: {
      entry: ({ context }) => {
        console.log('âœ… [Response Ready] LLM ì‘ë‹µ ìˆ˜ì‹ ');
        console.log(`\nì‚¬ìš©ì: ${context.userMessage}`);
        console.log(`ë´‡: ${context.currentResponse}\n`);
      },
      on: {
        SEND_MESSAGE: {
          target: 'validating',
          actions: assign({
            userMessage: ({ event }) => event.message,
            currentResponse: null,
            error: null,
            retryCount: 0
          })
        },
        RESET: {
          target: 'waiting_input',
          actions: assign({
            userMessage: '',
            conversationHistory: [],
            currentResponse: null,
            error: null,
            retryCount: 0
          })
        }
      }
    },
    error: {
      entry: ({ context }) => {
        console.log(`âŒ [Error] LLM í˜¸ì¶œ ì‹¤íŒ¨: ${context.error}`);
        console.log(`ì¬ì‹œë„ íšŸìˆ˜: ${context.retryCount}/${MAX_LLM_RETRIES}`);
      },
      /**
       * XState ì¬ì‹œë„ ì „ëµ: after + guard
       *
       * ì¥ì :
       * - ì¬ì‹œë„ ë¡œì§ì´ ìƒíƒœ ë¨¸ì‹ ì— ëª…ì‹œì ìœ¼ë¡œ í‘œí˜„ë¨
       * - ì‹œê°í™” ë„êµ¬ì—ì„œ ì¬ì‹œë„ ê²½ë¡œë¥¼ ë³¼ ìˆ˜ ìˆìŒ
       * - í…ŒìŠ¤íŠ¸í•˜ê¸° ì‰¬ì›€ (ê° ê²½ë¡œë¥¼ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥)
       */
      after: {
        [RETRY_DELAY_MS]: [
          {
            guard: ({ context }) => context.retryCount < MAX_LLM_RETRIES,
            target: 'calling_llm',
            actions: [
              assign({
                retryCount: ({ context }) => context.retryCount + 1,
                error: null
              }),
              ({ context }) => console.log(`ğŸ”„ [Retry ${context.retryCount + 1}] ì¬ì‹œë„ ì¤‘...`)
            ]
          },
          {
            target: 'failed'
          }
        ]
      },
      on: {
        RETRY: {
          target: 'calling_llm',
          actions: assign({
            retryCount: ({ context }) => context.retryCount + 1,
            error: null
          })
        }
      }
    },
    failed: {
      entry: () => {
        console.log('ğŸš« [Failed] ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼');
        console.log('ì‹œìŠ¤í…œì„ ì¬ì‹œì‘í•˜ê±°ë‚˜ ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
      },
      on: {
        RESET: {
          target: 'waiting_input',
          actions: assign({
            userMessage: '',
            conversationHistory: [],
            currentResponse: null,
            error: null,
            retryCount: 0
          })
        }
      },
      type: 'final'
    }
  }
});

// ì‹¤í–‰
async function runXStateChatbot() {
  console.log('=== XState Chatbot Workflow ===\n');
  console.log('íŠ¹ì§•: ëª…í™•í•œ ìƒíƒœ ì „ì´, íƒ€ì… ì•ˆì „ì„±, ì‹œê°í™” ê°€ëŠ¥\n');

  const chatActor = createActor(chatbotMachine);

  chatActor.subscribe((state) => {
    console.log(`[ìƒíƒœ ë³€ê²½] ${state.value}`);
  });

  chatActor.start();

  // ì‹œë®¬ë ˆì´ì…˜: 3ê°œì˜ ëŒ€í™”
  const messages = [
    'Hello!',
    'FSMì— ëŒ€í•´ ì•Œë ¤ì¤˜',
    'XStateëŠ” ì–´ë–¤ ì¥ì ì´ ìˆì–´?'
  ];

  for (let i = 0; i < messages.length; i++) {
    await new Promise(resolve => setTimeout(resolve, i * 3000));
    chatActor.send({ type: 'SEND_MESSAGE', message: messages[i] });
  }

  // ì¢…ë£Œ
  setTimeout(() => {
    console.log('\nâœ¨ XState ì±—ë´‡ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ\n');
    chatActor.stop();
    process.exit(0);
  }, 10000);
}

runXStateChatbot();
