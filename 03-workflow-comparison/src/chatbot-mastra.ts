/**
 * Mastraë¥¼ ì‚¬ìš©í•œ LLM ì±—ë´‡ ì›Œí¬í”Œë¡œìš°
 * AI í†µí•©ê³¼ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê°•ì¡°
 */

import { createStep, createWorkflow } from '@mastra/core';
import { z } from 'zod';
import { loadEnv, isOpenAIAvailable, callOpenAI } from './env.js';

// í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
const env = loadEnv();

// ìƒìˆ˜ ì •ì˜
const LLM_CALL_DELAY_MS = 1000;
const MAX_LLM_RETRIES = 3;
const LLM_SUCCESS_RATE = 0.8; // Mock ì „ìš©
const VALIDATION_DELAY_MS = 300;

// ëŒ€í™” ë©”ì‹œì§€ ìŠ¤í‚¤ë§ˆ
const messageSchema = z.object({
  role: z.enum(['user', 'assistant']),
  content: z.string()
});

// ì±—ë´‡ ìƒíƒœ ìŠ¤í‚¤ë§ˆ
const chatStateSchema = z.object({
  userMessage: z.string(),
  conversationHistory: z.array(messageSchema),
  currentResponse: z.string().nullable(),
  retryCount: z.number(),
  status: z.enum(['waiting', 'validating', 'processing', 'success', 'failed'])
});

type ChatState = z.infer<typeof chatStateSchema>;
type ChatInput = Partial<ChatState>;

const delay = (ms: number) => new Promise(resolve => setTimeout(resolve, ms));

/**
 * Mock LLM ì‘ë‹µ ìƒì„±
 */
const callLLMMock = async (
  conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }>
): Promise<string> => {
  console.log('ğŸ¤– [Calling LLM Mock] Mock API í˜¸ì¶œ ì¤‘...');
  await delay(LLM_CALL_DELAY_MS);

  if (Math.random() > LLM_SUCCESS_RATE) {
    throw new Error('LLM API í˜¸ì¶œ ì‹¤íŒ¨: Rate limit exceeded');
  }

  const lastUserMessage = conversationHistory
    .filter(msg => msg.role === 'user')
    .pop()?.content || '';

  const responses: Record<string, string> = {
    'hello': 'ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?',
    'fsm': 'FSM(Finite State Machine)ì€ ì‹œìŠ¤í…œì„ ëª…í™•í•œ ìƒíƒœë“¤ê³¼ ê·¸ë“¤ ê°„ì˜ ì „ì´ë¡œ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.',
    'mastra': 'MastraëŠ” AI í†µí•©ê³¼ ë³µì¡í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì— ìµœì í™”ëœ ì›Œí¬í”Œë¡œìš° í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. Zodë¥¼ ì‚¬ìš©í•œ ê°•ë ¥í•œ íƒ€ì… ê²€ì¦ê³¼ 40ê°œ ì´ìƒì˜ LLM ì œê³µìë¥¼ ì§€ì›í•©ë‹ˆë‹¤.',
    'workflow': 'Mastra ì›Œí¬í”Œë¡œìš°ëŠ” Stepì„ ì²´ì´ë‹í•˜ì—¬ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ëŠ” íŒŒì´í”„ë¼ì¸ ë°©ì‹ì…ë‹ˆë‹¤. ê° Stepì€ inputSchemaì™€ outputSchemaë¡œ íƒ€ì… ì•ˆì „ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.',
    'default': `"${lastUserMessage}"ì— ëŒ€í•œ ì§ˆë¬¸ì´êµ°ìš”. êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë¶€ë¶„ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?`
  };

  const lowerMessage = lastUserMessage.toLowerCase();
  for (const [key, value] of Object.entries(responses)) {
    if (lowerMessage.includes(key)) {
      return value;
    }
  }

  return responses['default'];
};

/**
 * LLM í˜¸ì¶œ (í™˜ê²½ ë³€ìˆ˜ì— ë”°ë¼ ì‹¤ì œ API ë˜ëŠ” Mock ì‚¬ìš©)
 */
const callLLM = async (
  conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }>
): Promise<string> => {
  if (isOpenAIAvailable()) {
    // ì‹¤ì œ OpenAI API í˜¸ì¶œ
    return await callOpenAI(conversationHistory, env.OPENAI_MODEL || 'gpt-4o-mini');
  } else {
    // Mock ì‘ë‹µ ë°˜í™˜
    return await callLLMMock(conversationHistory);
  }
};

/**
 * Step 1: ì…ë ¥ ê²€ì¦
 *
 * Mastraì˜ ê°•ì : Zod ìŠ¤í‚¤ë§ˆë¥¼ í™œìš©í•œ ëŸ°íƒ€ì„ íƒ€ì… ê²€ì¦
 * - ë¹ˆ ë©”ì‹œì§€ë‚˜ ì˜ëª»ëœ í˜•ì‹ì„ ì‚¬ì „ì— ì°¨ë‹¨
 * - inputSchemaì™€ outputSchemaê°€ ë°ì´í„° ê³„ì•½(contract) ì—­í• 
 */
const validateInput = createStep({
  id: 'validate-input',
  inputSchema: chatStateSchema.partial(),
  outputSchema: chatStateSchema,
  execute: async ({ inputData }) => {
    const input = inputData as ChatInput;
    console.log('ğŸ” [Validating] ì…ë ¥ ê²€ì¦ ì¤‘...');
    await delay(VALIDATION_DELAY_MS);

    const userMessage = input.userMessage?.trim() || '';

    if (userMessage.length === 0) {
      console.log('âš ï¸ [Validation Failed] ë¹ˆ ë©”ì‹œì§€ëŠ” ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤');
      return {
        userMessage: '',
        conversationHistory: input.conversationHistory || [],
        currentResponse: null,
        retryCount: 0,
        status: 'failed'
      } satisfies ChatState;
    }

    console.log(`âœ… [Validation Success] ë©”ì‹œì§€: "${userMessage}"`);
    return {
      userMessage,
      conversationHistory: input.conversationHistory || [],
      currentResponse: null,
      retryCount: input.retryCount || 0,
      status: 'validating'
    } satisfies ChatState;
  }
});

/**
 * Step 2: LLM í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
 *
 * Mastraì˜ ì¬ì‹œë„ ì „ëµ: Step ë‚´ë¶€ì—ì„œ while ë£¨í”„ ì‚¬ìš©
 *
 * ì¥ì :
 * - Stepì´ ë…ë¦½ì ìœ¼ë¡œ ì¬ì‹œë„ ë¡œì§ì„ ì™„ì „íˆ ì œì–´
 * - AI ì‘ì—…(LLM í˜¸ì¶œ)ì˜ ì¬ì‹œë„ì— íŠ¹íˆ ì í•©
 * - ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ëŠ” ë‹¨ìˆœí•˜ê²Œ ìœ ì§€ (ì¬ì‹œë„ëŠ” Stepì˜ ë‚´ë¶€ êµ¬í˜„)
 *
 * ë™ì‘ ë°©ì‹:
 * 1. ì„±ê³µí•˜ê±°ë‚˜ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µ
 * 2. ê° ì‹œë„ë§ˆë‹¤ LLM API í˜¸ì¶œ
 * 3. ì„±ê³µ ì‹œ ì¦‰ì‹œ break, ì‹¤íŒ¨ ì‹œ retryCount ì¦ê°€
 */
const callLLMStep = createStep({
  id: 'call-llm',
  inputSchema: chatStateSchema,
  outputSchema: chatStateSchema,
  execute: async ({ inputData }) => {
    let state: ChatState = { ...inputData };

    // ê²€ì¦ ì‹¤íŒ¨ ì‹œ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if (state.status === 'failed') {
      return state;
    }

    state.status = 'processing';
    let success = false;

    /**
     * ì¬ì‹œë„ ë£¨í”„
     *
     * - LLM APIëŠ” ì¼ì‹œì  ì¥ì• ê°€ ë¹ˆë²ˆí•¨ (Rate Limit, Timeout ë“±)
     * - ì¬ì‹œë„ë¥¼ í†µí•´ ì„±ê³µë¥ ì„ í¬ê²Œ ë†’ì¼ ìˆ˜ ìˆìŒ
     * - ê° ì‹œë„ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë¡œê¹…í•˜ì—¬ ë””ë²„ê¹… ìš©ì´
     */
    while (!success && state.retryCount < MAX_LLM_RETRIES) {
      const attempt = state.retryCount + 1;

      try {
        console.log(`ğŸ’³ [LLM Call] ì‹œë„ ${attempt}/${MAX_LLM_RETRIES}`);

        // ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        const updatedHistory = [
          ...state.conversationHistory,
          { role: 'user' as const, content: state.userMessage }
        ];

        // LLM í˜¸ì¶œ
        const response = await callLLM(updatedHistory);

        // ì„±ê³µ: ì‘ë‹µê³¼ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        console.log('ğŸ’° [LLM Success] ì‘ë‹µ ìƒì„± ì™„ë£Œ');
        state = {
          ...state,
          currentResponse: response,
          conversationHistory: [
            ...updatedHistory,
            { role: 'assistant' as const, content: response }
          ],
          status: 'success'
        };
        success = true;
      } catch (error) {
        console.log(`âŒ [LLM Failed] ${error instanceof Error ? error.message : String(error)}`);
        state = {
          ...state,
          retryCount: attempt
        };

        if (attempt < MAX_LLM_RETRIES) {
          console.log(`ğŸ”„ [Retry ${attempt}] ì¬ì‹œë„ ì¤€ë¹„...`);
          await delay(1500); // ì¬ì‹œë„ ì „ ëŒ€ê¸°
        }
      }
    }

    // ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
    if (!success) {
      console.log('ğŸš« [LLM Failed] ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼');
      state = {
        ...state,
        status: 'failed'
      };
    }

    return state;
  }
});

/**
 * Step 3: ì‘ë‹µ ì¶œë ¥
 *
 * Mastraì˜ íŒŒì´í”„ë¼ì¸ ì² í•™:
 * - ê° Stepì€ ë‹¨ì¼ ì±…ì„ (SRP: Single Responsibility Principle)
 * - ë°ì´í„° ë³€í™˜ê³¼ ë¶€ìˆ˜ íš¨ê³¼(ë¡œê¹…)ë¥¼ ë¶„ë¦¬
 */
const displayResponse = createStep({
  id: 'display-response',
  inputSchema: chatStateSchema,
  outputSchema: chatStateSchema,
  execute: async ({ inputData }) => {
    const state = inputData;

    if (state.status === 'success' && state.currentResponse) {
      console.log('\nâœ… [Response Ready] ì‘ë‹µ ì¤€ë¹„ ì™„ë£Œ');
      console.log(`ì‚¬ìš©ì: ${state.userMessage}`);
      console.log(`ë´‡: ${state.currentResponse}\n`);
    } else if (state.status === 'failed') {
      console.log('\nâš ï¸ [Error] ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤\n');
    }

    return state;
  }
});

/**
 * Mastra ì›Œí¬í”Œë¡œìš° êµ¬ì„±
 *
 * íŠ¹ì§•:
 * - then() ì²´ì´ë‹ìœ¼ë¡œ ì„ í˜•ì ì¸ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
 * - ê° Stepì˜ outputSchemaê°€ ë‹¤ìŒ Stepì˜ inputSchemaì™€ í˜¸í™˜ë˜ì–´ì•¼ í•¨
 * - commit()ìœ¼ë¡œ ì›Œí¬í”Œë¡œìš° ìµœì¢… í™•ì •
 */
const chatWorkflow = createWorkflow({
  id: 'chatbot-workflow',
  inputSchema: chatStateSchema.partial(),
  outputSchema: chatStateSchema
})
  .then(validateInput)
  .then(callLLMStep)
  .then(displayResponse)
  .commit();

// ì‹¤í–‰
async function runMastraChatbot() {
  console.log('=== Mastra Chatbot Workflow ===\n');
  console.log('íŠ¹ì§•: AI í†µí•©, ë°ì´í„° íŒŒì´í”„ë¼ì¸, Zod íƒ€ì… ê²€ì¦\n');

  const messages = [
    'Hello!',
    'FSMì— ëŒ€í•´ ì•Œë ¤ì¤˜',
    'Mastraì˜ ì¥ì ì€ ë­ì•¼?'
  ];
  let conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }> = [];

  for (const message of messages) {
    console.log(`\n--- ìƒˆë¡œìš´ ë©”ì‹œì§€ ì²˜ë¦¬: "${message}" ---\n`);

    try {
      const run = await chatWorkflow.createRunAsync();
      const result = await run.start({
        inputData: {
          userMessage: message,
          conversationHistory,
          retryCount: 0
        }
      });

      if (result.status === 'success') {
        console.log('--- ëŒ€í™” íˆìŠ¤í† ë¦¬ ---');
        result.result.conversationHistory.forEach((msg, idx) => {
          console.log(`${idx + 1}. [${msg.role}] ${msg.content}`);
        });
        conversationHistory = result.result.conversationHistory;
      } else if (result.status === 'failed') {
        console.error('ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨:', result.error);
      }
    } catch (error) {
      console.error('ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜:', error);
    }

    // ë‹¤ìŒ ë©”ì‹œì§€ ì „ ëŒ€ê¸°
    await delay(1000);
  }

  console.log('\nâœ¨ Mastra ì±—ë´‡ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ\n');
}

runMastraChatbot();
