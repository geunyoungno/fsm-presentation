/**
 * LangGraphë¥¼ ì‚¬ìš©í•œ LLM ì±—ë´‡ ì›Œí¬í”Œë¡œìš°
 * ì—ì´ì „íŠ¸ ê¸°ë°˜ ë™ì  ë¼ìš°íŒ…ê³¼ LLM í†µí•© ê°•ì¡°
 */

import { StateGraph, Annotation, END } from '@langchain/langgraph';
import { loadEnv, isOpenAIAvailable } from './env.js';
import { callOpenAI } from './openai-client.js';

// í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
const env = loadEnv();

// ìƒìˆ˜ ì •ì˜
const LLM_CALL_DELAY_MS = 1000;
const MAX_LLM_RETRIES = 3;
const LLM_SUCCESS_RATE = 0.8; // Mock ì „ìš©
const VALIDATION_DELAY_MS = 300;

// ëŒ€í™” ë©”ì‹œì§€ íƒ€ì…
interface Message {
  role: 'user' | 'assistant';
  content: string;
}

// LangGraph Annotationì„ ì‚¬ìš©í•œ ìƒíƒœ ì •ì˜
const ChatGraphAnnotation = Annotation.Root({
  userMessage: Annotation<string>,
  conversationHistory: Annotation<Message[]>,
  currentResponse: Annotation<string | null>,
  retryCount: Annotation<number>,
  status: Annotation<'waiting' | 'validating' | 'processing' | 'success' | 'failed'>
});

type ChatGraphState = typeof ChatGraphAnnotation.State;

const delay = (ms: number) => new Promise(resolve => setTimeout(resolve, ms));

/**
 * Mock LLM ì‘ë‹µ ìƒì„±
 */
const callLLMMock = async (
  conversationHistory: Message[]
): Promise<string> => {
  console.log('ğŸ¤– [Calling LLM Mock] Mock API í˜¸ì¶œ ì¤‘...');
  await delay(LLM_CALL_DELAY_MS);

  if (Math.random() > LLM_SUCCESS_RATE) {
    throw new Error('LLM API í˜¸ì¶œ ì‹¤íŒ¨: Rate limit exceeded');
  }

  const lastUserMessage = conversationHistory
    .filter(msg => msg.role === 'user')
    .pop()?.content || '';

  const responses: Record<string, string> = {
    'hello': 'ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?',
    'fsm': 'FSM(Finite State Machine)ì€ ì‹œìŠ¤í…œì„ ëª…í™•í•œ ìƒíƒœë“¤ê³¼ ê·¸ë“¤ ê°„ì˜ ì „ì´ë¡œ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.',
    'langgraph': 'LangGraphëŠ” LLM ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ì— íŠ¹í™”ëœ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ë™ì  ë¼ìš°íŒ…ê³¼ ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ í†µí•´ ë³µì¡í•œ ì—ì´ì „íŠ¸ í–‰ë™ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
    'agent': 'LangGraphì˜ ì—ì´ì „íŠ¸ëŠ” ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í–‰ë™ì„ ë™ì ìœ¼ë¡œ ê²°ì •í•©ë‹ˆë‹¤. ReAct íŒ¨í„´, ë„êµ¬ í˜¸ì¶œ, ë©€í‹° ì—ì´ì „íŠ¸ í˜‘ì—… ë“±ì„ ì§€ì›í•©ë‹ˆë‹¤.',
    'default': `"${lastUserMessage}"ì— ëŒ€í•œ ì§ˆë¬¸ì´êµ°ìš”. êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë¶€ë¶„ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?`
  };

  const lowerMessage = lastUserMessage.toLowerCase();
  for (const [key, value] of Object.entries(responses)) {
    if (lowerMessage.includes(key)) {
      return value;
    }
  }

  return responses['default'];
};

/**
 * LLM í˜¸ì¶œ (í™˜ê²½ ë³€ìˆ˜ì— ë”°ë¼ ì‹¤ì œ API ë˜ëŠ” Mock ì‚¬ìš©)
 */
const callLLM = async (
  conversationHistory: Message[]
): Promise<string> => {
  if (isOpenAIAvailable()) {
    // ì‹¤ì œ OpenAI API í˜¸ì¶œ
    return await callOpenAI(conversationHistory, env.OPENAI_MODEL || 'gpt-4o-mini');
  } else {
    // Mock ì‘ë‹µ ë°˜í™˜
    return await callLLMMock(conversationHistory);
  }
};

/**
 * ë…¸ë“œ 1: ì…ë ¥ ê²€ì¦
 */
async function validateInputNode(state: ChatGraphState): Promise<Partial<ChatGraphState>> {
  console.log('ğŸ” [Validating] ì…ë ¥ ê²€ì¦ ì¤‘...');
  await delay(VALIDATION_DELAY_MS);

  const userMessage = state.userMessage.trim();

  if (userMessage.length === 0) {
    console.log('âš ï¸ [Validation Failed] ë¹ˆ ë©”ì‹œì§€ëŠ” ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤');
    return {
      status: 'failed'
    };
  }

  console.log(`âœ… [Validation Success] ë©”ì‹œì§€: "${userMessage}"`);
  return {
    status: 'validating'
  };
}

/**
 * ë…¸ë“œ 2: LLM í˜¸ì¶œ
 *
 * LangGraphì˜ ì² í•™:
 * - ê° ë…¸ë“œëŠ” ë‹¨ì¼ ì‘ì—…ë§Œ ìˆ˜í–‰ (ì—¬ê¸°ì„œëŠ” LLM í˜¸ì¶œ 1íšŒ)
 * - ì¬ì‹œë„ëŠ” ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ í†µí•´ ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìˆ˜ì¤€ì—ì„œ ì²˜ë¦¬
 * - ë…¸ë“œëŠ” ìˆœìˆ˜í•˜ê³  í…ŒìŠ¤íŠ¸í•˜ê¸° ì‰¬ì›€
 */
async function callLLMNode(state: ChatGraphState): Promise<Partial<ChatGraphState>> {
  const attempt = state.retryCount + 1;
  console.log(`ğŸ’³ [LLM Call] ì‹œë„ ${attempt}/${MAX_LLM_RETRIES}`);

  try {
    // ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    const updatedHistory: Message[] = [
      ...state.conversationHistory,
      { role: 'user', content: state.userMessage }
    ];

    // LLM í˜¸ì¶œ
    const response = await callLLM(updatedHistory);

    // ì„±ê³µ
    console.log('ğŸ’° [LLM Success] ì‘ë‹µ ìƒì„± ì™„ë£Œ');
    return {
      currentResponse: response,
      conversationHistory: [
        ...updatedHistory,
        { role: 'assistant', content: response }
      ],
      status: 'success'
    };
  } catch (error) {
    console.log(`âŒ [LLM Failed] ${error instanceof Error ? error.message : String(error)}`);
    return {
      retryCount: attempt,
      status: 'processing' // ì¬ì‹œë„ ê°€ëŠ¥ ìƒíƒœ
    };
  }
}

/**
 * ë…¸ë“œ 3: ì‘ë‹µ ì¶œë ¥
 */
async function displayResponseNode(state: ChatGraphState): Promise<Partial<ChatGraphState>> {
  console.log('\nâœ… [Response Ready] ì‘ë‹µ ì¤€ë¹„ ì™„ë£Œ');
  console.log(`ì‚¬ìš©ì: ${state.userMessage}`);
  console.log(`ë´‡: ${state.currentResponse}\n`);

  return {};
}

/**
 * ë…¸ë“œ 4: ì‹¤íŒ¨ ì²˜ë¦¬
 */
async function handleFailureNode(state: ChatGraphState): Promise<Partial<ChatGraphState>> {
  console.log('\nğŸš« [Failed] ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼');
  console.log('ì‹œìŠ¤í…œì„ ì¬ì‹œì‘í•˜ê±°ë‚˜ ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n');
  return {
    status: 'failed'
  };
}

/**
 * LangGraphì˜ í•µì‹¬: ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ í†µí•œ ë™ì  ë¼ìš°íŒ…
 *
 * ì¬ì‹œë„ ì „ëµ:
 * - ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
 * - LLM í˜¸ì¶œ ì‹¤íŒ¨ + ì¬ì‹œë„ ê°€ëŠ¥ â†’ call_llmìœ¼ë¡œ ë‹¤ì‹œ ëŒì•„ê° (ë£¨í”„)
 * - LLM í˜¸ì¶œ ì‹¤íŒ¨ + ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼ â†’ handle_failureë¡œ ì´ë™
 * - LLM í˜¸ì¶œ ì„±ê³µ â†’ display_responseë¡œ ì§„í–‰
 *
 * ì¥ì :
 * - ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìì²´ê°€ ì¬ì‹œë„ ê²½ë¡œë¥¼ ëª…ì‹œì ìœ¼ë¡œ í‘œí˜„
 * - ì‹œê°í™”í–ˆì„ ë•Œ íë¦„ì„ ì‰½ê²Œ ì´í•´ ê°€ëŠ¥
 * - ê° ë…¸ë“œëŠ” ë‹¨ìˆœí•œ ì‘ì—…ë§Œ ìˆ˜í–‰ (ê´€ì‹¬ì‚¬ì˜ ë¶„ë¦¬)
 */
function routeAfterValidation(state: ChatGraphState): string {
  if (state.status === 'failed') {
    return END; // ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
  }
  return 'call_llm'; // ê²€ì¦ ì„±ê³µ ì‹œ LLM í˜¸ì¶œë¡œ ì§„í–‰
}

function routeAfterLLMCall(state: ChatGraphState): string {
  // LLM í˜¸ì¶œ ì„±ê³µ
  if (state.status === 'success') {
    return 'display_response';
  }

  // LLM í˜¸ì¶œ ì‹¤íŒ¨ + ì¬ì‹œë„ ê°€ëŠ¥
  if (state.status === 'processing' && state.retryCount < MAX_LLM_RETRIES) {
    console.log(`ğŸ”„ [Retry ${state.retryCount}] ì¬ì‹œë„ ì¤€ë¹„...`);
    return 'call_llm'; // ê°™ì€ ë…¸ë“œë¡œ ë‹¤ì‹œ ëŒì•„ê° (ë£¨í”„)
  }

  // ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
  return 'handle_failure';
}

/**
 * LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±
 *
 * íŠ¹ì§•:
 * - ë…¸ë“œì™€ ì—£ì§€ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì—°ê²°
 * - ì¡°ê±´ë¶€ ì—£ì§€ë¡œ ë™ì  ë¼ìš°íŒ… êµ¬í˜„
 * - ê·¸ë˜í”„ ì‹œê°í™” ê°€ëŠ¥ (Mermaid, ì‹œê°í™” ë„êµ¬ ë“±)
 */
const workflow = new StateGraph(ChatGraphAnnotation)
  // ë…¸ë“œ ì¶”ê°€
  .addNode('validate_input', validateInputNode)
  .addNode('call_llm', callLLMNode)
  .addNode('display_response', displayResponseNode)
  .addNode('handle_failure', handleFailureNode)
  // ì—£ì§€ ì—°ê²°
  .addEdge('__start__', 'validate_input')
  // ì¡°ê±´ë¶€ ì—£ì§€: ê²€ì¦ í›„ ë¼ìš°íŒ…
  .addConditionalEdges('validate_input', routeAfterValidation)
  // ì¡°ê±´ë¶€ ì—£ì§€: LLM í˜¸ì¶œ í›„ ë¼ìš°íŒ… (ì¬ì‹œë„ ë¡œì§)
  .addConditionalEdges('call_llm', routeAfterLLMCall)
  // ì‘ë‹µ ì¶œë ¥ í›„ ì¢…ë£Œ
  .addEdge('display_response', '__end__')
  // ì‹¤íŒ¨ ì²˜ë¦¬ í›„ ì¢…ë£Œ
  .addEdge('handle_failure', '__end__');

// ê·¸ë˜í”„ ì»´íŒŒì¼
const app = workflow.compile();

// ì‹¤í–‰
async function runLangGraphChatbot() {
  console.log('=== LangGraph Chatbot Workflow ===\n');
  console.log('íŠ¹ì§•: ì—ì´ì „íŠ¸ ê¸°ë°˜, ë™ì  ë¼ìš°íŒ…, LLM í†µí•©\n');

  const messages = [
    'Hello!',
    'FSMì— ëŒ€í•´ ì•Œë ¤ì¤˜',
    'LangGraphì˜ íŠ¹ì§•ì€ ë­ì•¼?'
  ];
  let conversationHistory: Message[] = [];

  for (const message of messages) {
    console.log(`\n--- ìƒˆë¡œìš´ ë©”ì‹œì§€ ì²˜ë¦¬: "${message}" ---\n`);

    try {
      const initialState: ChatGraphState = {
        userMessage: message,
        conversationHistory,
        currentResponse: null,
        retryCount: 0,
        status: 'waiting'
      };

      const result = await app.invoke(initialState);

      if (result.status === 'success') {
        console.log('--- ëŒ€í™” íˆìŠ¤í† ë¦¬ ---');
        result.conversationHistory.forEach((msg, idx) => {
          console.log(`${idx + 1}. [${msg.role}] ${msg.content}`);
        });
        conversationHistory = result.conversationHistory;
      } else if (result.status === 'failed') {
        console.log('ì›Œí¬í”Œë¡œìš°ê°€ ì‹¤íŒ¨ ìƒíƒœë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.');
      }
    } catch (error) {
      console.error('ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜:', error);
    }

    // ë‹¤ìŒ ë©”ì‹œì§€ ì „ ëŒ€ê¸°
    await delay(1000);
  }

  console.log('\nâœ¨ LangGraph ì±—ë´‡ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ\n');
}

runLangGraphChatbot();
